{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N-SIzbvRamWt"
   },
   "source": [
    "# PROBLEM 1\n",
    "In this problem you are required to apply various clustering techniques on a given dataset\n",
    "SyntheticQ1.csv, which is an artificial dataset containing 4 convex clusters. The dataset\n",
    "contains two attributes (X and Y) for each instance, delimited by semicolons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G1mXD6-Ya2b3"
   },
   "source": [
    "a. Preprocess the dataset removing the records that contain any missing value (left empty or\n",
    "marked with ‘?’ in the dataset) and removing any record that has a negative value of X or Y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "x1iP3NPHYjq2",
    "outputId": "e75ec249-6e40-4148-e1a1-82b3fa01c714"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the dataset\n",
    "df = pd.read_csv('SyntheticQ1.csv', delimiter=';')\n",
    "\n",
    "\n",
    "# Convert 'X' and 'Y' columns to numeric, handling errors by coercing to NaN\n",
    "df['X'] = pd.to_numeric(df['X'], errors='coerce')\n",
    "df['Y'] = pd.to_numeric(df['Y'], errors='coerce')\n",
    "\n",
    "# Drop rows with missing values\n",
    "df = df.replace('?', pd.NA)\n",
    "df = df.dropna()\n",
    "\n",
    "# Remove records with negative(-) values of X or Y\n",
    "df = df[(df['X'] >= 0) & (df['Y'] >= 0)]\n",
    "\n",
    "# Check preprocessed dataset\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "rIZMLas04SlH",
    "outputId": "ffe9aa6d-d23d-4929-b932-5f4a035912d6"
   },
   "outputs": [],
   "source": [
    "#Normalizing the Data\n",
    "normalized_df = (df - df.min()) / (df.max() - df.min())\n",
    "normalized_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6_mUbzagbHfZ"
   },
   "source": [
    "b. Apply the K-means algorithm on the pre-processed dataset to generate 4 clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vmCNLTIBbOAE",
    "outputId": "752f5e90-5e24-4281-cb30-815487f2fb49"
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Apply K-means with 4 clusters\n",
    "kmeans = KMeans(n_clusters = 4, n_init = 50, verbose = 0)\n",
    "labels = kmeans.fit_predict(normalized_df)\n",
    "\n",
    "# Print the dataset with K-means clusters\n",
    "print(labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gk9I2jwk3JO_"
   },
   "source": [
    "c. Visualize the clusters of part a. using scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "wLJtG6eQ3L8x",
    "outputId": "b2a63c0e-b237-4e05-8bd1-f25638649a11"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Add the 'Cluster' column to the normalized DataFrame\n",
    "normalized_df['Cluster'] = labels\n",
    "\n",
    "# Scatter plot\n",
    "plt.scatter(normalized_df['X'], normalized_df['Y'], c=normalized_df['Cluster'], cmap='viridis')\n",
    "plt.title('K-means Clustering')\n",
    "plt.xlabel('Normalized X')\n",
    "plt.ylabel('Normalized Y')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0zaxufGKBAkd"
   },
   "source": [
    "d. Apply DBSCAN on the pre-processed dataset with ε = 0.5 and minPts = 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t3QLbACLBBjs",
    "outputId": "951e3dfb-bfd5-45c2-9710-819f12d5c4b5"
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "dbscan = DBSCAN(eps = 0.5, min_samples = 3)\n",
    "\n",
    "labels = dbscan.fit_predict(df)\n",
    "\n",
    "print(labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EJSFA7MXC_K2"
   },
   "source": [
    "e. Visualize the clusters of part c. using scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "cOlVnWskDCnO",
    "outputId": "0f0b18f0-fc41-40e8-ea44-d47b8719af54"
   },
   "outputs": [],
   "source": [
    "normalized_df['Cluster'] = labels\n",
    "\n",
    "plt.scatter(normalized_df['X'], normalized_df['Y'], c=normalized_df['Cluster'], cmap='viridis')\n",
    "plt.title('DBSCAN Clustering')\n",
    "plt.xlabel('Normalized X')\n",
    "plt.ylabel('Normalized Y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nrC_toxFDOUv"
   },
   "source": [
    "f. Apply single-linkage hierarchical clustering on the pre-processed dataset to generate 4\n",
    "partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JnYEvaXMEKT9",
    "outputId": "c36e2aff-fceb-43ce-a5fb-7ac9b466d0f4"
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "# Apply single-linkage hierarchical clustering with 4 clusters\n",
    "hierarchical = AgglomerativeClustering(n_clusters=4, affinity = 'euclidean', linkage='single')\n",
    "labels_hierarchical = hierarchical.fit_predict(normalized_df)\n",
    "\n",
    "# Print the dataset hierarchical clustering labels\n",
    "print(labels_hierarchical)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mhqfX8s-nuyG"
   },
   "source": [
    "g. Visualize the clusters of part e. using scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "YTvQG2TNnv3q",
    "outputId": "c3827989-188f-43ee-9c0b-c7f08874a1d1"
   },
   "outputs": [],
   "source": [
    "normalized_df['Cluster'] = labels_hierarchical\n",
    "\n",
    "plt.scatter(normalized_df['X'], normalized_df['Y'], c=normalized_df['Cluster'], cmap='viridis')\n",
    "plt.title('Single-Linkage Hierarchical Clustering')\n",
    "plt.xlabel('Normalized_X')\n",
    "plt.ylabel('Normalized_Y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SNxITR0kpkRU"
   },
   "source": [
    "h. Apply complete-linkage hierarchical clustering on the pre-processed dataset to generate 4\n",
    "partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IQrn8IPwpk_E",
    "outputId": "764be162-9428-4191-b2d1-5894757d110a"
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "# Apply complete-linkage hierarchical clustering with 4 clusters\n",
    "hierarchical = AgglomerativeClustering(n_clusters=4, affinity = 'euclidean', linkage='complete')\n",
    "labels_hierarchical = hierarchical.fit_predict(normalized_df)\n",
    "\n",
    "# Print the dataset hierarchical clustering labels\n",
    "print(labels_hierarchical)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lm5IRTPOp3qP"
   },
   "source": [
    "i. Visualize the clusters of part g. using scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "Oi--yCTOp493",
    "outputId": "788c4a71-3f7c-4e72-cf4e-9f7519b8f35d"
   },
   "outputs": [],
   "source": [
    "normalized_df['Cluster'] = labels_hierarchical\n",
    "\n",
    "plt.scatter(normalized_df['X'], normalized_df['Y'], c=normalized_df['Cluster'], cmap='viridis')\n",
    "plt.title('Complete-Linkage Hierarchical Clustering')\n",
    "plt.xlabel('Normalized_X')\n",
    "plt.ylabel('Normalized_Y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iBndzwNOqEoP"
   },
   "source": [
    "j. Apply average-linkage hierarchical clustering on the pre-processed dataset to generate 4\n",
    "partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Zh6T3ggdqFXs",
    "outputId": "3e772223-0f1b-4449-8a6f-d348c6e54a2d"
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "# Apply complete-linkage hierarchical clustering with 4 clusters\n",
    "hierarchical = AgglomerativeClustering(n_clusters=4, affinity = 'euclidean', linkage='average')\n",
    "labels_hierarchical = hierarchical.fit_predict(normalized_df)\n",
    "\n",
    "# Print the dataset hierarchical clustering labels\n",
    "print(labels_hierarchical)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "guOj-tbdqH_Z"
   },
   "source": [
    "k. Visualize the clusters of part i. using scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "4HotjLIfqKxP",
    "outputId": "ccc567ef-bcab-417c-b0df-dc476696c233"
   },
   "outputs": [],
   "source": [
    "normalized_df['Cluster'] = labels_hierarchical\n",
    "\n",
    "plt.scatter(normalized_df['X'], normalized_df['Y'], c=normalized_df['Cluster'], cmap='viridis')\n",
    "plt.title('Average-Linkage Hierarchical Clustering')\n",
    "plt.xlabel('Normalized_X')\n",
    "plt.ylabel('Normalized_Y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ms5gyqViqnVQ"
   },
   "source": [
    "l. Briefly compare and explain the outcomes of the previous parts of this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eSDcFtjrqpGv"
   },
   "source": [
    "The dataset we're working with exhibits convex clusters, causing KMeans to struggle with accurate clustering due to its limitations in handling non-globular and differently sized clusters.\n",
    "\n",
    "DBSCAN demonstrated effective clustering by leveraging the close densities of our clusters and optimal parameter tuning, eliminating the need to specify the cluster count. In contrast, Hierarchical clustering, though successful, required us to specify the desired number of clusters. Despite its proficiency, this method consumed more time and computational resources than necessary, given that our dataset's nature favors partitional clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1qU2vMmuubMh"
   },
   "source": [
    "# PROBLEM 2\n",
    "In this problem you are required to apply various clustering techniques on a given dataset\n",
    "seeds.csv, which contains 4 attributes of various plant seeds: the length of the seed, the width of\n",
    "the seed, asymmetry coefficient of the seed and the compactness coefficient of the seed. The\n",
    "dataset contains a header and the values are delimited by semicolons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lDKtSbwlurf2"
   },
   "source": [
    "a. Apply the \"elbow\" (a.k.a \"knee\") rule to find the optimal number of clusters for this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 504
    },
    "id": "wije6pgwutig",
    "outputId": "356b9f3a-16a9-4157-d362-edf55fc68098"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Read the dataset we loaded\n",
    "df = pd.read_csv('seeds.csv', delimiter=';')\n",
    "\n",
    "# Normalizing Data\n",
    "normalized_df = (df - df.min()) / (df.max() - df.min())\n",
    "\n",
    "# Identifying the optimum number of clusters (Elbow Method)\n",
    "minNumClusters = 1\n",
    "maxNumClusters = 15\n",
    "wcss = [] #Errorr\n",
    "\n",
    "for k in range(minNumClusters, maxNumClusters+1):\n",
    "  kmeans = KMeans(n_clusters = k, n_init = 50)\n",
    "  kmeans.fit_predict(df)\n",
    "  wcss.append(kmeans.inertia_)\n",
    "\n",
    "print(\"Generated errors are:\\n\",wcss)\n",
    "\n",
    "# 4.2. Visualizing Errors vs Numbers of Clusters\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(range(minNumClusters, maxNumClusters+1), wcss, '-o')\n",
    "ax.set_xlabel(\"Number of Clusters (k)\")\n",
    "ax.set_ylabel(\"Error\")\n",
    "ax.set_xticks(range(minNumClusters, maxNumClusters+1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uXlq8vq6w_6y"
   },
   "source": [
    "answer: 6 clusters is the optimal number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rizeB-7_xr22"
   },
   "source": [
    "b. Apply the K-means algorithm on this dataset with the number of clusters found in part a."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qu-tkUqexsMh",
    "outputId": "6869f495-95c4-410c-b9a9-43c1a3b2cae4"
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Apply K-means with 6 clusters\n",
    "kmeans = KMeans(n_clusters = 6, n_init = 50, verbose = 0)\n",
    "labels = kmeans.fit_predict(normalized_df)\n",
    "\n",
    "# Print to Check the dataset with K-means clusters\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2a2Wdq_Yx3av"
   },
   "source": [
    "c. Visualize the clusters using scatter plot (employing dimensionality reduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "XZGAq11Qx4PX",
    "outputId": "fe7ea6a0-49ba-4b9b-f049-b9316cc49158"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Fiting the PCA model to Normalized df\n",
    "pca = PCA(n_components=2).fit(normalized_df)\n",
    "pca_2d = pca.transform(normalized_df)\n",
    "\n",
    "#Visualizing Clusters\n",
    "plt.scatter(pca_2d[:, 0], pca_2d[:, 1], c=labels, cmap='viridis')\n",
    "\n",
    "plt.title(\"Visualization of KMeans\")\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yg_ZmbHVzoCB"
   },
   "source": [
    "d. Draw the heatmap of this clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 452
    },
    "id": "Yef9f7xazo88",
    "outputId": "bf97056b-e28b-4e79-d5ba-2131c2ab3d7a"
   },
   "outputs": [],
   "source": [
    "import sklearn.metrics as metrics\n",
    "\n",
    "normalized_df['clusterLabels'] = labels\n",
    "normalized_df_sorted = normalized_df.sort_values(by=['clusterLabels'])\n",
    "\n",
    "euclidean_dists = metrics.euclidean_distances(normalized_df_sorted)\n",
    "plt.pcolormesh(euclidean_dists,cmap='hot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g95blbwy01pE"
   },
   "source": [
    "e. Apply (at least one variant of) hierarchical clustering on this dataset to generate K partitions\n",
    "(where K is the value found in part a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GqF83M7U02m9",
    "outputId": "831bb7bd-48af-4ec9-bae5-3fb7d18b7e6f"
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "# Normalizing Data\n",
    "normalized_df = (df - df.min()) / (df.max() - df.min())\n",
    "\n",
    "# Apply single-linkage hierarchical clustering with 6 clusters\n",
    "hierarchical = AgglomerativeClustering(n_clusters=6, affinity = 'euclidean', linkage='single')\n",
    "labels = hierarchical.fit_predict(normalized_df)\n",
    "\n",
    "# Print the dataset hierarchical clustering labels to check\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eow3hZLU1BDH"
   },
   "source": [
    "f. Visualize the clusters using scatter plot (employing dimensionality reduction)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 593
    },
    "id": "LfUnWBET1BYt",
    "outputId": "c64abaff-316c-4668-90f5-7b37d3f321cc"
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2).fit(normalized_df)\n",
    "pca_2d = pca.transform(normalized_df)\n",
    "print(pca_2d)\n",
    "\n",
    "plt.scatter(pca_2d[:, 0], pca_2d[:, 1], c = hierarchical.labels_, cmap = 'viridis')\n",
    "plt.title(\"Visualization of Single-linkage Hierarchical Clustering\")\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iFWrPkhWBvBK"
   },
   "source": [
    "g. Draw the heatmap of this clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 452
    },
    "id": "C8JYbdPHBvlf",
    "outputId": "fdaa558e-224e-48f9-9cec-0b5d5c5718be"
   },
   "outputs": [],
   "source": [
    "import sklearn.metrics as metrics\n",
    "\n",
    "normalized_df['clusterLabels'] = labels\n",
    "normalized_df_sorted = normalized_df.sort_values(by=['clusterLabels'])\n",
    "\n",
    "euclidean_dists = metrics.euclidean_distances(normalized_df_sorted)\n",
    "plt.pcolormesh(euclidean_dists,cmap='hot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ryiKfOKcIkrP"
   },
   "source": [
    "h. Briefly compare the results of K-means and hierarchical clustering for this dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kl_13PG5Imff"
   },
   "source": [
    "In this dataset, the effectiveness of KMeans clustering is attributed to the existence of clearly defined spherical clusters. While hierarchical clustering also exhibits satisfactory performance, KMeans emerges as the preferred option for this dataset, mainly due to its computational efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3pDBsLi4KU4t"
   },
   "source": [
    "# PROBLEM 3\n",
    "You are given the dataset stones.csv which contains data about the height, width, density,\n",
    "compactness and texture of some mineral stones. For each stone, in the first column is given the\n",
    "class it belongs to (A, B, C, D, E or F)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CxENRXjeKYwX"
   },
   "source": [
    "a. Split the dataset randomly into 60% train and 40% test and build a classification model based\n",
    "on decision trees. Generate the confusion matrix and classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vqR2QUOSPqPo",
    "outputId": "7f7020f1-6785-430a-cfae-1edbe842c49f"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Read the dataset\n",
    "data = pd.read_csv('stones.csv', delimiter=',')\n",
    "\n",
    "# Convert columns to numeric, handling errors by coercing to NaN\n",
    "data['Height'] = pd.to_numeric(data['Height'], errors='coerce')\n",
    "data['Width'] = pd.to_numeric(data['Width'], errors='coerce')\n",
    "data['Density'] = pd.to_numeric(data['Density'], errors='coerce')\n",
    "data['Compactness'] = pd.to_numeric(data['Compactness'], errors='coerce')\n",
    "data['Texture'] = pd.to_numeric(data['Texture'], errors='coerce')\n",
    "\n",
    "# Drop rows with missing values\n",
    "data = data.dropna()\n",
    "\n",
    "# Dividing Features from Class\n",
    "x = data.iloc[:, 1:6]\n",
    "y = data.iloc[:, 0]\n",
    "\n",
    "# Splitting the DataSet into Train and Test\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, train_size=0.60)\n",
    "\n",
    "# Creating Classification Object and fit the data to create the model\n",
    "classifier = DecisionTreeClassifier()\n",
    "classifier.fit(x_train, y_train)\n",
    "\n",
    "# Predicting the labels of new instances (test sub dataset)\n",
    "y_predicted = classifier.predict(x_test)\n",
    "\n",
    "# Checking Classification Effectiveness\n",
    "print(confusion_matrix(y_test, y_predicted), end=\"\\n\\n\")\n",
    "print(classification_report(y_test, y_predicted))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MSEoxnWfVBZ0"
   },
   "source": [
    "b. Split the dataset randomly into 60% train and 40% test and build a classification model based\n",
    "on KNN with K=5. Generate the confusion matrix and classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9gRrVlh1VLAR",
    "outputId": "79bbecde-511c-44bd-d40d-5d67b778ad5e"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Read the dataset\n",
    "data = pd.read_csv('stones.csv', delimiter=',')\n",
    "\n",
    "# Convert columns to numeric, handling errors by coercing to NaN\n",
    "data['Height'] = pd.to_numeric(data['Height'], errors='coerce')\n",
    "data['Width'] = pd.to_numeric(data['Width'], errors='coerce')\n",
    "data['Density'] = pd.to_numeric(data['Density'], errors='coerce')\n",
    "data['Compactness'] = pd.to_numeric(data['Compactness'], errors='coerce')\n",
    "data['Texture'] = pd.to_numeric(data['Texture'], errors='coerce')\n",
    "\n",
    "# Drop rows with missing values\n",
    "data = data.dropna()\n",
    "\n",
    "# Dividing Features from Class\n",
    "x = data.iloc[:, 1:6]\n",
    "y = data.iloc[:, 0]\n",
    "\n",
    "# Splitting the DataSet into Train and Test\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, train_size=0.60)\n",
    "\n",
    "# Creating Classification Object and fit the data to create the model\n",
    "classifier = KNeighborsClassifier(n_neighbors=5)\n",
    "classifier.fit(x_train, y_train)\n",
    "\n",
    "# Predicting the labels of new instances (test sub dataset)\n",
    "y_predicted = classifier.predict(x_test)\n",
    "\n",
    "# Print Classification Effectiveness\n",
    "print(confusion_matrix(y_test, y_predicted), end=\"\\n\\n\")\n",
    "print(classification_report(y_test, y_predicted))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UkfyMN25WDzV"
   },
   "source": [
    "c. Split the dataset randomly into 60% train and 40% test and build a classification model based\n",
    "on SVM with polynomial kernel of degree 3. Generate the confusion matrix and classification\n",
    "report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ecgtx0LfWZd2",
    "outputId": "26db40c6-fe38-465c-df58-d09341580fbe"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Read the loaded dataset\n",
    "data = pd.read_csv('stones.csv', delimiter=',')\n",
    "\n",
    "# Convert columns to numeric, handling errors by coercing to NaN\n",
    "data['Height'] = pd.to_numeric(data['Height'], errors='coerce')\n",
    "data['Width'] = pd.to_numeric(data['Width'], errors='coerce')\n",
    "data['Density'] = pd.to_numeric(data['Density'], errors='coerce')\n",
    "data['Compactness'] = pd.to_numeric(data['Compactness'], errors='coerce')\n",
    "data['Texture'] = pd.to_numeric(data['Texture'], errors='coerce')\n",
    "\n",
    "# Drop rows with missing values\n",
    "data = data.dropna()\n",
    "\n",
    "# Dividing Features from Class\n",
    "x = data.iloc[:, 1:6]\n",
    "y = data.iloc[:, 0]\n",
    "\n",
    "# Splitting the DataSet into Train and Test\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, train_size=0.60)\n",
    "\n",
    "# Creating Classification Object and fit the data to create the model\n",
    "classifier = SVC(kernel = 'poly', degree = 3)\n",
    "classifier.fit(x_train, y_train)\n",
    "\n",
    "# Predicting the labels of new instances (test sub dataset)\n",
    "y_predicted = classifier.predict(x_test)\n",
    "\n",
    "# Checking Classification Effectiveness\n",
    "print(confusion_matrix(y_test, y_predicted), end=\"\\n\\n\")\n",
    "print(classification_report(y_test, y_predicted))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nvvZlfuuYt0i"
   },
   "source": [
    "d. Use all the above techniques to classify a new entry with height = 6.4, width = 4.15, density =\n",
    "7.1, compactness = 8.8 and texture = 7.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5zhusfKBYv1b",
    "outputId": "24970239-928c-425c-bcc0-ec14cb9d072b"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Read the dataset\n",
    "data = pd.read_csv('stones.csv', delimiter=',')\n",
    "\n",
    "# Convert columns to numeric, handling errors by coercing to NaN\n",
    "data['Height'] = pd.to_numeric(data['Height'], errors='coerce')\n",
    "data['Width'] = pd.to_numeric(data['Width'], errors='coerce')\n",
    "data['Density'] = pd.to_numeric(data['Density'], errors='coerce')\n",
    "data['Compactness'] = pd.to_numeric(data['Compactness'], errors='coerce')\n",
    "data['Texture'] = pd.to_numeric(data['Texture'], errors='coerce')\n",
    "\n",
    "# Drop rows with missing values\n",
    "data = data.dropna()\n",
    "\n",
    "# Dividing Features from Class\n",
    "x = data.iloc[:, 1:6]\n",
    "y = data.iloc[:, 0]\n",
    "\n",
    "# Train Decision Tree Classifier\n",
    "dt_classifier = DecisionTreeClassifier()\n",
    "dt_classifier.fit(x, y)\n",
    "\n",
    "# Training KNN Classifier\n",
    "knn_classifier = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_classifier.fit(x, y)\n",
    "\n",
    "# Training SVM Classifier with polynomial kernel of degree 3\n",
    "svm_classifier = SVC(kernel='poly', degree=3)\n",
    "svm_classifier.fit(x, y)\n",
    "\n",
    "# New entry data\n",
    "new_entry = pd.DataFrame({\n",
    "    'Height': [6.4],\n",
    "    'Width': [4.15],\n",
    "    'Density': [7.1],\n",
    "    'Compactness': [8.8],\n",
    "    'Texture': [7.5]\n",
    "})\n",
    "\n",
    "# Predicting with Decision Tree\n",
    "dt_prediction = dt_classifier.predict(new_entry)\n",
    "print(\"Decision Tree Prediction:\", dt_prediction[0])\n",
    "\n",
    "# Predicting with KMeans\n",
    "knn_prediction = knn_classifier.predict(new_entry)\n",
    "print(\"KNN Prediction:\", knn_prediction[0])\n",
    "\n",
    "# Predicting with SVM\n",
    "svm_prediction = svm_classifier.predict(new_entry)\n",
    "print(\"SVM Prediction:\", svm_prediction[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zC7XUCvZagCr"
   },
   "source": [
    "# PROBLEM 4\n",
    "In this problem you are required to apply various classification techniques on a benchmark\n",
    "dataset, spambase.data, from the UCI repository. This dataset contains 57 attributes, where the\n",
    "last one is the class: spam (1) or non-spam (0). For further details you may visit:\n",
    "https://archive.ics.uci.edu/ml/datasets/spambase\n",
    "Obtain 500 random splits of the dataset into training (80%) and test (20%) and for each split\n",
    "apply all these classification techniques:\n",
    "i. Decision trees\n",
    "ii. KNN\n",
    "iii. Support Vector Machines\n",
    "iv. Logistic Regression\n",
    "v. Naïve Bayes\n",
    "Print a summarization table showing the average values of precision, recall, f1 score and\n",
    "accuracy, which are obtained from the 500 tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nyqSsxyEahqM"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "# Read data after download it fromm the link\n",
    "data = pd.read_csv('spambase.data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zTmzqp1YgqWL",
    "outputId": "4ef1ff57-116f-41ba-eeb9-11d52c3032d0"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the dataset\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data\"\n",
    "data = np.loadtxt(url, delimiter=\",\")\n",
    "features = data[:, :-1]\n",
    "labels = data[:, -1]\n",
    "\n",
    "# Number of random splits\n",
    "num_splits = 500\n",
    "\n",
    "# Lists to store metrics for each classifier\n",
    "decision_tree_metrics = []\n",
    "knn_metrics = []\n",
    "svm_metrics = []\n",
    "logistic_regression_metrics = []\n",
    "naive_bayes_metrics = []\n",
    "\n",
    "# Loop over random splits\n",
    "for _ in range(num_splits):\n",
    "    # Split the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=None)\n",
    "\n",
    "    # Decision Tree\n",
    "    dt_classifier = DecisionTreeClassifier()\n",
    "    dt_classifier.fit(X_train, y_train)\n",
    "    dt_preds = dt_classifier.predict(X_test)\n",
    "    decision_tree_metrics.append((precision_score(y_test, dt_preds),\n",
    "                                  recall_score(y_test, dt_preds),\n",
    "                                  f1_score(y_test, dt_preds),\n",
    "                                  accuracy_score(y_test, dt_preds)))\n",
    "\n",
    "    # KNN\n",
    "    knn_classifier = KNeighborsClassifier()\n",
    "    knn_classifier.fit(X_train, y_train)\n",
    "    knn_preds = knn_classifier.predict(X_test)\n",
    "    knn_metrics.append((precision_score(y_test, knn_preds),\n",
    "                        recall_score(y_test, knn_preds),\n",
    "                        f1_score(y_test, knn_preds),\n",
    "                        accuracy_score(y_test, knn_preds)))\n",
    "\n",
    "    # Support Vector Machines with scaling\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    svm_classifier = SVC()\n",
    "    svm_classifier.fit(X_train[:1000], y_train[:1000])  # Using the first 1000 samples for training\n",
    "    svm_preds = svm_classifier.predict(X_test)\n",
    "    svm_metrics.append((precision_score(y_test, svm_preds),\n",
    "                    recall_score(y_test, svm_preds),\n",
    "                    f1_score(y_test, svm_preds),\n",
    "                    accuracy_score(y_test, svm_preds)))\n",
    "\n",
    "\n",
    "    # Logistic Regression with scaling\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    lr_classifier = LogisticRegression(max_iter=1000)\n",
    "    lr_classifier.fit(X_train_scaled, y_train)\n",
    "    lr_preds = lr_classifier.predict(X_test_scaled)\n",
    "    logistic_regression_metrics.append((precision_score(y_test, lr_preds),\n",
    "                                        recall_score(y_test, lr_preds),\n",
    "                                        f1_score(y_test, lr_preds),\n",
    "                                        accuracy_score(y_test, lr_preds)))\n",
    "\n",
    "    # Naïve Bayes\n",
    "    nb_classifier = GaussianNB()\n",
    "    nb_classifier.fit(X_train, y_train)\n",
    "    nb_preds = nb_classifier.predict(X_test)\n",
    "    naive_bayes_metrics.append((precision_score(y_test, nb_preds),\n",
    "                                recall_score(y_test, nb_preds),\n",
    "                                f1_score(y_test, nb_preds),\n",
    "                                accuracy_score(y_test, nb_preds)))\n",
    "\n",
    "# Calculate average metrics\n",
    "avg_decision_tree_metrics = np.mean(decision_tree_metrics, axis=0)\n",
    "avg_knn_metrics = np.mean(knn_metrics, axis=0)\n",
    "avg_svm_metrics = np.mean(svm_metrics, axis=0)\n",
    "avg_lr_metrics = np.mean(logistic_regression_metrics, axis=0)\n",
    "avg_nb_metrics = np.mean(naive_bayes_metrics, axis=0)\n",
    "\n",
    "# Print the summarization table\n",
    "print(\"Classifier\\tPrecision\\tRecall\\tF1 Score\\tAccuracy\")\n",
    "print(\"Decision Tree\\t{:.4f}\\t\\t{:.4f}\\t{:.4f}\\t\\t{:.4f}\".format(*avg_decision_tree_metrics))\n",
    "print(\"KNN\\t\\t{:.4f}\\t\\t{:.4f}\\t{:.4f}\\t\\t{:.4f}\".format(*avg_knn_metrics))\n",
    "print(\"SVM\\t\\t{:.4f}\\t\\t{:.4f}\\t{:.4f}\\t\\t{:.4f}\".format(*avg_svm_metrics))\n",
    "print(\"Logistic Regression\\t{:.4f}\\t\\t{:.4f}\\t{:.4f}\\t\\t{:.4f}\".format(*avg_lr_metrics))\n",
    "print(\"Naive Bayes\\t{:.4f}\\t\\t{:.4f}\\t{:.4f}\\t\\t{:.4f}\".format(*avg_nb_metrics))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q4n3vQ0V4ur9"
   },
   "source": [
    "# Problem 5\n",
    "\n",
    "In this problem you are required to pick a benchmark dataset (from UCI repository or other\n",
    "authoritative resources), partition it into train and test components and apply various\n",
    "classification techniques. For each classification technique, you should display in a common\n",
    "plot how the accuracy, precision, recall and f1 score are varying for different ratios of train/test\n",
    "of the original dataset. (Note: there will be a different graph for each classification technique.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "gYtuBXYU4_P_",
    "outputId": "a50f2e65-4fa3-4aa5-e817-6ee744b42a2f"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd  # Import pandas for reading CSV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Load the Heart dataset from CSV\n",
    "heart_data = pd.read_csv('heart.csv')\n",
    "X, y = heart_data.iloc[:, :-1].values, heart_data.iloc[:, -1].values\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Define classification techniques\n",
    "classifiers = {\n",
    "    'Decision Tree': DecisionTreeClassifier(),\n",
    "    'KNN': KNeighborsClassifier(),\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000),  # Increase max_iter\n",
    "    'Naive Bayes': GaussianNB(),\n",
    "    'SVM': SVC()\n",
    "}\n",
    "\n",
    "# Define different train/test ratios\n",
    "ratios = [0.6, 0.7, 0.8, 0.9]\n",
    "\n",
    "# Initialize plots\n",
    "fig, axes = plt.subplots(nrows=len(classifiers), ncols=1, figsize=(8, 4 * len(classifiers)))\n",
    "\n",
    "# Iterate through classifiers\n",
    "for i, (clf_name, clf) in enumerate(classifiers.items()):\n",
    "    # Initialize lists to store metrics\n",
    "    accuracy_list, precision_list, recall_list, f1_list = [], [], [], []\n",
    "\n",
    "    # Iterate through ratios\n",
    "    for ratio in ratios:\n",
    "        # Split the scaled dataset\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=1 - ratio, random_state=42)\n",
    "\n",
    "        # Train the classifier\n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "        # Make predictions\n",
    "        y_pred = clf.predict(X_test)\n",
    "\n",
    "        # Calculate evaluation metrics\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred, average='weighted')\n",
    "        recall = recall_score(y_test, y_pred, average='weighted')\n",
    "        f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "        # Append metrics to lists\n",
    "        accuracy_list.append(accuracy)\n",
    "        precision_list.append(precision)\n",
    "        recall_list.append(recall)\n",
    "        f1_list.append(f1)\n",
    "\n",
    "    # Plot the metrics for each classifier\n",
    "    axes[i].plot(ratios, accuracy_list, label='Accuracy')\n",
    "    axes[i].plot(ratios, precision_list, label='Precision')\n",
    "    axes[i].plot(ratios, recall_list, label='Recall')\n",
    "    axes[i].plot(ratios, f1_list, label='F1 Score')\n",
    "    axes[i].set_title(f'{clf_name} Performance vs Train/Test Ratio')\n",
    "    axes[i].set_xlabel('Train/Test Ratio')\n",
    "    axes[i].set_ylabel('Score')\n",
    "    axes[i].legend()\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
